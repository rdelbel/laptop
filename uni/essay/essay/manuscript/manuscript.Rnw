\documentclass{article}
\usepackage{multirow}
\setlength\parindent{0pt}
\usepackage{geometry}
\usepackage{longtable}
\usepackage{float}

%This is word 'normal' margins
\geometry{left=1.25in,right=1in,top=1.25in,bottom=1.25in}
%word 'moderate' margins
%\geometry{left=.75in,right=.75in,top=1in,bottom=1in}
%word 'narrow' margins
%\geometry{left=.5in,right=.5in,top=.5in,bottom=.5in}



%Change title here
\title{Preprocessing with SVM}
%Change Author here
\author{Ryan Del Bel}

\begin{document}
\maketitle


<<,include=FALSE>>=
require(MASS)
require(ggplot2)
require(e1071)
require(reshape)
require(multicore)
require(Hmisc)
require(ROCR)

set.seed(123456789)
auc<-function (y, phat)
{
  # the two arguments are:
  # y = list of actual binary labels, 0, 1;
  # phat = list of ranking (or predicted probability) that an item is a 1 rather than a 0
  y <- as.numeric(y)
  phat <- as.numeric(phat)
  prediction(phat, y)->obj
  performance(obj,'auc')->junk
  as.numeric(attr(junk,'y.values'))
}

uncorrelated<-function(totalfeat=200,realfeat=10,nobs=500,beta=1.2,constant=F,...){
  #print(shift)
  #print(constant)
  truecovs=data.frame(matrix(rnorm(nobs*realfeat),ncol=realfeat))
  if(!constant) betas=seq(beta[1],beta[2],length.out=realfeat)
  z=apply(truecovs,1,function(x){
    z=rep(0,length(x))
    for (i in 1:length(x)) z[i]=x[i]*ifelse(constant,log(beta),log(betas[i]))
    sum(z)
  })
  pr=1/(1+exp(-z))
  Y=rbinom(nobs,1,pr)
  noise=mvrnorm(nobs,rnorm(totalfeat-realfeat),diag(totalfeat-realfeat))
  data=data.frame(Y,truecovs,noise)
  colnames(data)[-1]=sapply(1:(ncol(data)-1),function(x) paste0("X",x))
  return(data)
}
fitsvm<-function(svlimit=0,...){
  points=uncorrelated(...)
 #cname=colnames(points)
  #points$y=factor(points$y)
  #plot(ggplot(points,aes(x1,x2,color=factor(y)))+geom_point())
  #svm.model=tune.svm(y~.,data=points,type="C-classification",kernel="linear")
  svm.model=svm(Y~.,data=points,type="C-classification",kernel="linear",cost=1)
  sv=svm.model$index
  svl=round(svlimit/2,0)
 
   big=which(svm.model$coefs==1)
   small=which(svm.model$coefs==-1)
   if(svlimit){
   if(svl<length(big)) big=sample(big,svl)
   if(svl<length(small)) small=sample(small,svl)
   }
   remove=c(big,small)
 
  sv=sv[remove]
  #sv=svm.model$best.model$index
 # print(nobs-length(sv))
  results=cbind(t(sapply(colnames(points)[-1],function(var){
    f=as.formula(paste0(var,"~Y"))
    c(t.test(f,data=points)$p.value,
      t.test(f,data=points[!1:nrow(points)%in%sv,])$p.value)
  })))
  #rownames(results)=1:length(cname[-1])
  colnames(results)=c("Normal","SVM")
 results=melt(results)
 results$sv=length(sv)
 return(results)
}
simresult<-function(df,topn=10,realfeat=10,...){
  df$X1=as.numeric(substring(df$X1,2))
  colnames(df)=c("feature","type","pvalue","sv")
  df=df[order(df$type,df$pvalue),]
  df$y=ifelse(df$feature<=realfeat,1,0)
  df$nltp=-log10(df$pvalue)
  dfsvm=subset(df,df$type=="SVM")
  dfnormal=subset(df,df$type=="Normal")
  aucresult=c(auc(dfsvm$y,dfsvm$nltp),auc(dfnormal$y,dfnormal$nltp))
  fweresult=c(sum(dfsvm$feature[1:topn]<=realfeat),sum(dfnormal$feature[1:topn]<=realfeat))
  fweresult20=c(sum(dfsvm$feature[1:20]<=realfeat),sum(dfnormal$feature[1:20]<=realfeat))

  c(aucresult,fweresult,fweresult20,df$sv[1])
}
simulatesvm<-function(n,...){
  result=mclapply(1:n,function(x) fitsvm(...),mc.cores=8)
#print(result)
  result=matrix(unlist(mclapply(result,function(x)simresult(x,...),mc.cores=8)),ncol=7,byrow=T)
  apply(result,2,mean)
}



sim<-function(n,betas,constant=F,svlimit=0){
  OR=sapply(betas,function(x)simulatesvm(n=n,constant=constant,beta=x,svlimit=svlimit))
  OR=data.frame(t(OR))
colnames(OR)=c("SVM","Normal","SVM","Normal","SVM","Normal","\\#SV")
OR=round(OR,3)
#print(betas)
if(!constant) rownames(OR)=sapply(betas,function(x) paste0(x[1],"-",x[2])) else rownames(OR)= betas
return(OR)
}
@

<<,include=FALSE,cache=TRUE>>=
OR=sim(100,seq(1,2,.1),T)
OR2=sim(100,list(c(1.1,1.5),c(1.1,2),c(1.5,2),c(2,2.5)))
@

<<,include=FALSE,cache=TRUE>>=
OR3=sim(100,seq(1,2,.1),T,5)
OR4=sim(100,list(c(1.1,1.5),c(1.1,2),c(1.5,2)),F,5)
OR5=sim(100,seq(1,2,.1),T,10)
OR6=sim(100,list(c(1.1,1.5),c(1.1,2),c(1.5,2)),F,10)
OR7=sim(100,seq(1,1.6,.1),T,25)
OR8=sim(100,list(c(1.1,1.5),c(1.1,2)),F,25)
OR9=sim(100,seq(1,1.4,.1),T,50)
OR10=sim(100,list(c(1.1,1.5)),F,50)

tbl=rbind(OR3,OR4,OR5,OR6,OR7,OR8,OR9,OR10)
tbl$OR=rownames(tbl)
tbl$OR[15:43]=sapply(rownames(tbl)[15:43],function(x)substring(x,1,nchar(x)-1))
tbl$OR[37]="1.1-1."
colnames(tbl)=c("OR","SVM","Normal","SVM","Normal","SVM","Normal","\\#SV")

@


\section{Simulations with uncorrelated features}

We first consider the simple case with uncorrelated features. We let n=500, simulate 10 truly associated features and 190 noisy features. The effect of the features on the outcome was achieved by setting the odds ratio in the underlying logistic model. For simplicity and efficiency when using the linear SVM we simply set the tuning parameter $C$ to be one. Since the number of support vectors tends to be very large we only remove the support vectors that have maximal weights (i.e. with absolute value 1). For each feature we obtain the p-value from the t-test on the full sample and the sample with the support vectors with maximal weight removed. We then calculate the AUC, number of truly associated features that are in the top 10, 20 smallest p-values, and the number of support vectors removed. We replicate these simulations 100 times and report the average of each of the above statistics. \\

We first simulate data with the 10 associated features having the same OR.


<<,echo=FALSE,results='asis'>>=
latex(OR,file="",cgroup = c("AUC", "TOP10","TOP20",""), n.cgroup = c(2, 2,2,1),here=T)
@


Next we simulate 10 associated features with OR evenly spaced between a specified minimum and maximum OR.

<<,echo=FALSE,results='asis'>>=
latex(OR2,file="",cgroup = c("AUC", "TOP10","TOP20",""), n.cgroup = c(2, 2,2,1),here=T)
@

In both cases the results are slightly worse when using SVM as a preprocessing step. We notice that even when only removing the support vectors with maximal weight, we still usually remove a very large number of support vectors. We will now limit the number of support vectors we remove to $k$. If there are more than $k$ support vectors with maximal weight we then randomly remove $k/2$ support vectors with weight 1 and $k/2$ support vectors with weight -1.

<<,echo=FALSE,results='asis'>>=
tbl=tbl[-37,c(8,1:7)]
colnames(tbl)=c("OR","SVM","Normal","SVM","Normal","SVM","Normal","\\#SV")


latex(tbl,file="",cgroup = c("","AUC", "TOP10","TOP20",""),
      rgroup=c("1\\%","2\\%","5\\%","10\\%"), n.cgroup = c(1,2, 2,2,1),
      n.rgroup=c(14,14,8,6),here=T,rowlabel="",rowname=NULL)

@

Even when limiting the number of support vectors we remove, the normal approach still performs better.

\end{document}